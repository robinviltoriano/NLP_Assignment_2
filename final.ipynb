{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import faiss\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>PARIS  ?   When the Islamic State was about to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Angels are everywhere in the Mu?iz family?s ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>WASHINGTON  ?   It?s   or   time for Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            article\n",
       "0  17307  PARIS  ?   When the Islamic State was about to...\n",
       "1  17292  Angels are everywhere in the Mu?iz family?s ap...\n",
       "2  17298  Finally. The Second Avenue subway opened in Ne...\n",
       "3  17311  WASHINGTON  ?   It?s   or   time for Republica...\n",
       "4  17339  For Megyn Kelly, the shift from Fox News to NB..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset\n",
    "df = pd.read_csv('news_dataset.csv', encoding='latin-1')\n",
    "data = df[['id', 'article']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>17313</td>\n",
       "      <td>The body of the Iraqi prisoner was found naked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>17545</td>\n",
       "      <td>DETROIT  ?   Just before the holidays, on a da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>17546</td>\n",
       "      <td>DETROIT  ?   Just before the holidays, on a da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>18185</td>\n",
       "      <td>The body of the Iraqi prisoner was found naked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>18186</td>\n",
       "      <td>The body of the Iraqi prisoner was found naked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>18337</td>\n",
       "      <td>HOUSTON  ?   The chants rang out loud and long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>18338</td>\n",
       "      <td>HOUSTON  ?   The chants rang out loud and long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>18339</td>\n",
       "      <td>Picking the pain reliever that?s best for you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>18341</td>\n",
       "      <td>Picking the pain reliever that?s best for you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            article\n",
       "41   17313  The body of the Iraqi prisoner was found naked...\n",
       "219  17545  DETROIT  ?   Just before the holidays, on a da...\n",
       "220  17546  DETROIT  ?   Just before the holidays, on a da...\n",
       "752  18185  The body of the Iraqi prisoner was found naked...\n",
       "753  18186  The body of the Iraqi prisoner was found naked...\n",
       "886  18337  HOUSTON  ?   The chants rang out loud and long...\n",
       "887  18338  HOUSTON  ?   The chants rang out loud and long...\n",
       "888  18339  Picking the pain reliever that?s best for you ...\n",
       "889  18341  Picking the pain reliever that?s best for you ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "data[data.duplicated(subset=['article'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>PARIS  ?   When the Islamic State was about to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Angels are everywhere in the Mu?iz family?s ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>WASHINGTON  ?   It?s   or   time for Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            article\n",
       "0  17307  PARIS  ?   When the Islamic State was about to...\n",
       "1  17292  Angels are everywhere in the Mu?iz family?s ap...\n",
       "2  17298  Finally. The Second Avenue subway opened in Ne...\n",
       "3  17311  WASHINGTON  ?   It?s   or   time for Republica...\n",
       "4  17339  For Megyn Kelly, the shift from Fox News to NB..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "data_no_dup = data.drop_duplicates(subset=['article'],keep='first').reset_index(drop=True)\n",
    "data_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074 32759\n"
     ]
    }
   ],
   "source": [
    "# Check length of documents\n",
    "data_no_dup['len_article'] = data_no_dup['article'].apply(lambda x:len(x))\n",
    "min_len = data_no_dup['len_article'].min()\n",
    "max_len = data_no_dup['len_article'].max()\n",
    "print(min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x81', '\\x90', '\\x9b', '\\x9c', '\\x9f', 'ª', '\\xad', '®', 'ÿ'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check non-ASCII characters\n",
    "def check_non_ascii(text):\n",
    "    text = ''.join([char for char in text if ord(char) > 128])\n",
    "    return text\n",
    "\n",
    "check_set = set()\n",
    "for i in range(len(data_no_dup)):\n",
    "    check = check_non_ascii(data_no_dup['article'][i])\n",
    "    for char in check:\n",
    "        check_set.add(char)\n",
    "check_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to clean data\n",
    "def clean_text(text):\n",
    "    # Remove non-ASCII characters\n",
    "    text = ''.join([char for char in text if ord(char) < 128])\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove question mark problems\n",
    "    text = re.sub(r'(\\s\\?)',' ',text)\n",
    "    text = re.sub(r\"\\b\\?\\b\", \"\\'\", text)\n",
    "    text = re.sub(r\"(,\\?)\",\",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunking function\n",
    "def chunk_text(data_index, data_text, chunk_size, chunk_overlap):\n",
    "\n",
    "    list_chunk_text = []\n",
    "\n",
    "    for position in range(len(data_index)):\n",
    "        \n",
    "        # Clean data\n",
    "        words = clean_text(data_text[position]).split()\n",
    "\n",
    "        # Chunk data\n",
    "        start = 0\n",
    "        part = 1\n",
    "        while start < len(words):\n",
    "            end = start + chunk_size\n",
    "            segment = ' '.join(words[start:end])\n",
    "            list_chunk_text.append((str(data_index[position]) + str(part), segment))\n",
    "            part += 1\n",
    "            start += (chunk_size - chunk_overlap)\n",
    "\n",
    "    return pd.DataFrame(list_chunk_text, columns=['id', 'article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "data_chunk_train = chunk_text(data_no_dup[:5]['id'], data_no_dup[:5]['article'], 100, 50)\n",
    "data_chunk_train.to_csv('data_chunk_train.csv', index=False)\n",
    "\n",
    "# Create testing dataset\n",
    "data_no_train = data_no_dup[5:].reset_index(drop=True)\n",
    "data_chunk = chunk_text(data_no_train['id'], data_no_train['article'], 100, 50)\n",
    "data_chunk.to_csv('data_chunk_100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model to encode data\n",
    "model = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')\n",
    "\n",
    "# Vectorize data\n",
    "encoded_data = model.encode(data_chunk['article'].tolist())\n",
    "encoded_data = np.asarray(encoded_data.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vector database\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
    "index.add_with_ids(encoded_data, np.array(range(0, len(data_chunk))))\n",
    "faiss.write_index(index, 'data_article_100.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieval Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')\n",
    "cross_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to fetch data\n",
    "def fetch_data(doc_ids, score):\n",
    "    \n",
    "    '''doc_ids should be a list of document ids'''\n",
    "    info = data_chunk.iloc[doc_ids]\n",
    "    meta_dict = {}\n",
    "    meta_dict['id'] = info['id']\n",
    "    meta_dict['article'] = info['article']\n",
    "    meta_dict['score'] = score\n",
    "\n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to search top k match documents for query\n",
    "def search(query, top_k, index, model):\n",
    "\n",
    "    query_vector = model.encode([query])\n",
    "    top_k = index.search(query_vector, top_k)\n",
    "\n",
    "    top_k_ids = list(top_k[1].tolist()[0])\n",
    "    score = list(top_k[0].tolist()[0])\n",
    "\n",
    "    results =  [fetch_data(idx, score) for idx, score in zip(top_k_ids, score)]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to retrieve top k documents with cross-encoder\n",
    "def query_answer(query, query_id):\n",
    "    \"\"\"Query is a string\"\"\"\n",
    "    query = clean_text(query)\n",
    "\n",
    "    # Search top 20 related documents\n",
    "    results = search(query, top_k=20, index=index, model=model)\n",
    "\n",
    "    # Re-rank the results\n",
    "    model_inputs = [[query, result['article']] for result in results]\n",
    "    scores = cross_model.predict(model_inputs)\n",
    "\n",
    "    # Sort the scores in decreasing order\n",
    "    ranked_results = [{'id': result['id'], 'article': result['article'], 'score': score} for result, score in zip(results, scores)]\n",
    "    ranked_results = sorted(ranked_results, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Display top 3 results\n",
    "    result_dataset = []\n",
    "    for i, rank in enumerate(ranked_results[:3]):\n",
    "        dataset = {'question_id': query_id,\n",
    "                   'question': query,\n",
    "                   'rank': i + 1,\n",
    "                   'id': int(rank['id'] // 10),\n",
    "                   'article': rank['article'],\n",
    "                   'score': rank['score']}\n",
    "        result_dataset.append(dataset)\n",
    "\n",
    "    return result_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read vector database\n",
    "index = faiss.read_index('data_article_100.index')\n",
    "data_chunk = pd.read_csv('data_chunk_100.csv')\n",
    "question_dataset = pd.read_csv('question_test_data_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to calculate MRR\n",
    "def mrr_score(answer_data, question_data):\n",
    "    '''answer_data is a list of list of ids\n",
    "    question_data has 2 columns: question and correct document ids'''\n",
    "    score = []\n",
    "    for i, answer in enumerate(answer_data):\n",
    "        for j, index in enumerate(answer):\n",
    "            if index == question_data[i]:\n",
    "                score.append(1 / (j + 1))\n",
    "                break\n",
    "        if len(score) < i + 1:\n",
    "            score.append(0)\n",
    "    return sum(score) / len(score) if len(score) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8243243243243243"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get re-rank id lists for each question\n",
    "answer_100_dataset = dict()\n",
    "for idx, question in enumerate(question_dataset['question']):\n",
    "    answers = query_answer(question, idx)\n",
    "    for answer in answers:\n",
    "        if answer['question_id'] not in answer_100_dataset:\n",
    "            answer_100_dataset[answer['question_id']] = [answer['id']]\n",
    "        else:\n",
    "            answer_100_dataset[answer['question_id']].append(answer['id'])\n",
    "\n",
    "article_100_ids = [answer_100_dataset[x] for x in answer_100_dataset]\n",
    "\n",
    "# Calculate MRR score\n",
    "score = mrr_score(article_100_ids, question_dataset['doc_id'])\n",
    "print('MRR score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with chunking 500-word dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk 500-word dataset\n",
    "data_chunk = chunk_text(data_no_train['id'], data_no_train['article'], 500, 50)\n",
    "data_chunk.to_csv('data_chunk_500.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vector database\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
    "index.add_with_ids(encoded_data, np.array(range(0, len(data_chunk))))\n",
    "faiss.write_index(index, 'data_article_500.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get re-rank id lists for each question\n",
    "answer_500_dataset = dict()\n",
    "for idx, question in enumerate(question_dataset['question']):\n",
    "    answers = query_answer(question, idx)\n",
    "    for answer in answers:\n",
    "        if answer['question_id'] not in answer_500_dataset:\n",
    "            answer_500_dataset[answer['question_id']] = [answer['id']]\n",
    "        else:\n",
    "            answer_500_dataset[answer['question_id']].append(answer['id'])\n",
    "\n",
    "article_500_ids = [answer_500_dataset[x] for x in answer_500_dataset]\n",
    "\n",
    "# Calculate MRR score\n",
    "score = mrr_score(article_500_ids, question_dataset['doc_id'])\n",
    "print('MRR score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cross-encoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of cross-encoder models is used to test\n",
    "cross_models = ['cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "                'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "                'cross-encoder/ms-marco-MiniLM-L-4-v2',\n",
    "                'cross-encoder/ms-marco-MiniLM-L-2-v2',\n",
    "                'cross-encoder/ms-marco-TinyBERT-L-6',\n",
    "                'cross-encoder/ms-marco-TinyBERT-L-2-v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to test cross-encoder models\n",
    "def test_model(question_dataset, cross_model):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get re-rank id lists for each question\n",
    "    answer_dataset = dict()\n",
    "    for idx, question in enumerate(question_dataset['question']):\n",
    "        answers = query_answer(question, idx)\n",
    "        for answer in answers:\n",
    "            if answer['question_id'] not in answer_dataset:\n",
    "                answer_dataset[answer['question_id']] = [answer['id']]\n",
    "            else:\n",
    "                answer_dataset[answer['question_id']].append(answer['id'])\n",
    "\n",
    "    article_ids = [answer_dataset[x] for x in answer_dataset]\n",
    "    \n",
    "    # Calculate MRR score\n",
    "    mrr_score = mrr_score(article_ids, question_dataset['doc_id'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    result = {'mrr_score': article_ids,\n",
    "              'time': elapsed_time}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each cross-encoder model\n",
    "test_result = []\n",
    "for cross_model_name in cross_models:\n",
    "    # Set cross-encoder model\n",
    "    cross_model = CrossEncoder(cross_model_name) \n",
    "\n",
    "    # Test cross-encoder model\n",
    "    result = test_model(question_dataset, cross_model)\n",
    "    result['cross_model'] = cross_model_name    \n",
    "    test_result.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the positions of the bars on the x-axis\n",
    "x = range(len(cross_models))\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax1 = plt.subplots()\n",
    "x = range(len(cross_models))\n",
    "\n",
    "# Create bar charts for MRR score and time\n",
    "ax1.bar(x, test_result['mrr_score'], width=0.35, label='MRR score', color='b')\n",
    "ax1.set_ylabel('MRR score', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar([i + 0.35 for i in x], test_result['time'], width=0.35, label='Time', color='r')\n",
    "ax2.set_ylabel('Time', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Cross-encoder models performance comparison')\n",
    "plt.xlabel('Cross-encoder models')\n",
    "plt.xticks([i + 0.35/2 for i in x], cross_models, rotation=90)  # Adjust x-axis tick positions\n",
    "\n",
    "# Add legend\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
