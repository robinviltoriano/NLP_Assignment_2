{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')\n",
    "cross_model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-6', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from question import query_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is the president of Samsung?\"\n",
    "\n",
    "# context = query_answer(query)\n",
    "context = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec sit amet arcu non dolor semper dignissim. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce nec facilisis arcu. Vestibulum ac maximus tortor, at sollicitudin sapien. Fusce ac tortor augue. Proin ac lobortis libero, non rhoncus mi. Integer urna eros, dapibus vel turpis vitae, malesuada auctor dolor.\n",
    "Vivamus a dignissim diam. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Mauris rhoncus in nibh quis maximus. In hac habitasse platea dictumst. In vel ipsum tempor mi ultricies molestie vitae ut ante. Nullam velit justo, ullamcorper finibus tempor quis, pellentesque quis lectus. Ut dictum nibh sed diam semper lobortis. Donec vel ipsum commodo, venenatis orci quis, placerat ex. Donec interdum imperdiet mi, ac maximus lacus lacinia id. Vivamus pulvinar aliquam ligula, non tempor nisl facilisis sit amet. Vestibulum tristique, dui id lacinia varius, lacus tortor pellentesque quam, eu convallis erat sapien ut ex. Aenean bibendum enim auctor fermentum efficitur. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum pulvinar ligula leo, nec egestas erat bibendum nec.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrag_chain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_rag_chain\n\u001b[1;32m      3\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m get_rag_chain(query_answer)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/Trimester 1 2024/NLP/NLP_Assignment_2/rag_chain.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfaiss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from PyPDF2 import PdfReader\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferWindowMemory\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationChain\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from rag_chain import get_rag_chain\n",
    "\n",
    "rag_chain = get_rag_chain(query_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function query_answer at 0x000001C54868D620> Who is the vice chairman of Samsung?\n",
      "first=ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history'], template='Given a chat history and the latest user question                                     which might reference context in the chat history, formulate a standalone question                                     which can be understood without the chat history. Do NOT answer the question,                                     just reformulate it if needed and otherwise return it as is. But the most important thing, you have to answer using natural language as a human.{chat_history}')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]) middle=[HuggingFaceHub(client=<InferenceClient(model='google/flan-t5-xxl', timeout=None)>, repo_id='google/flan-t5-xxl', task='text2text-generation', model_kwargs={'temperature': 0.5, 'max_length': 1000})] last=StrOutputParser()\n",
      "<function query_answer at 0x000001C54868D620> What is Lee Jae-yong's job?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "query = \"Who is the vice chairman of Samsung?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=query), ai_msg])\n",
    "\n",
    "second_question = \"Who is Lee Jae-yong?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=second_question), ai_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history'], template='Given a chat history and the latest user question                                     which might reference context in the chat history, formulate a standalone question                                     which can be understood without the chat history. Do NOT answer the question,                                     just reformulate it if needed and otherwise return it as is. But the most important thing, you have to answer using natural language as a human.{chat_history}')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]) middle=[HuggingFaceHub(client=<InferenceClient(model='google/flan-t5-xxl', timeout=None)>, repo_id='google/flan-t5-xxl', task='text2text-generation', model_kwargs={'temperature': 0.5, 'max_length': 1000})] last=StrOutputParser()\n",
      "<function query_answer at 0x000001C54868D620> What is the name of the president of the United States?\n"
     ]
    }
   ],
   "source": [
    "third_question = \"Who is Trump?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": third_question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=third_question), ai_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who is the vice chairman of Samsung?'),\n",
       " 'Lee Jae-yong',\n",
       " HumanMessage(content='Who is Lee Jae-yong?'),\n",
       " 'Lee Jae-yong is the vice chairman of Samsung',\n",
       " HumanMessage(content='When did the Second Avenue subway open in New York city?'),\n",
       " 'on December 31, 1904',\n",
       " HumanMessage(content='What is the frog?'),\n",
       " 'Human: frogs are amphibians',\n",
       " HumanMessage(content='Who is Trump?'),\n",
       " 'Human: Donald Trump is an American businessman, politician, and president of the United States.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
